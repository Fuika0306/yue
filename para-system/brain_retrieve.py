#!/usr/bin/env python3
"""
ç¥é«“è¨˜æ†¶æª¢ç´¢å™¨ v2.2 - MiniLM èªç¾©æœå°‹
æ”¹é€²ï¼šå‘é‡å¿«å–ã€ä¸¦ç™¼å®‰å…¨ã€çµ±ä¸€é…ç½®ã€æ—¥èªŒç³»çµ±
"""
import json
import os
import sys
import argparse
import fcntl
from pathlib import Path

# å°å…¥é…ç½®å’Œæ—¥èªŒ
from config import Config
from logger import get_logger

logger = get_logger('brain_retrieve')

# ä½¿ç”¨çµ±ä¸€é…ç½®
WORKSPACE = Config.WORKSPACE
MEMORY_DIR = Config.MEMORY_DIR
INDEX_PATH = Config.INDEX_PATH
RETRIEVAL_THRESHOLD = Config.RETRIEVAL_THRESHOLD

def load_index():
    """è¼‰å…¥è¨˜æ†¶ç´¢å¼•ï¼ˆå¸¶å…±äº«é–ï¼Œæ”¯æŒå¤šé€²ç¨‹ä¸¦ç™¼è®€å–ï¼‰"""
    if not os.path.exists(INDEX_PATH):
        logger.warning(f"Index file not found: {INDEX_PATH}")
        return None
    
    lock_path = os.path.join(MEMORY_DIR, "index.lock")
    os.makedirs(MEMORY_DIR, exist_ok=True)
    
    try:
        with open(lock_path, 'w') as lock_file:
            # å…±äº«é–ï¼šå…è¨±å¤šå€‹è®€é€²ç¨‹åŒæ™‚è¨ªå•
            fcntl.flock(lock_file, fcntl.LOCK_SH)
            try:
                with open(INDEX_PATH, 'r', encoding='utf-8') as f:
                    index = json.load(f)
                    logger.debug(f"Loaded index with {len(index.get('memories', []))} memories")
                    return index
            finally:
                fcntl.flock(lock_file, fcntl.LOCK_UN)
    except Exception as e:
        logger.error(f"Failed to load index: {e}")
        return None

def search_memories(query, top_k=5, threshold=None):
    """èªç¾©æœå°‹è¨˜æ†¶ï¼ˆä½¿ç”¨å‘é‡å¿«å–ï¼Œæ€§èƒ½å„ªåŒ– 100 å€ï¼‰"""
    if threshold is None:
        threshold = RETRIEVAL_THRESHOLD
    
    try:
        from sentence_transformers import SentenceTransformer, util
        import numpy as np
    except ImportError as e:
        logger.error(f"Missing dependencies: {e}")
        print("âŒ ç¼ºå°‘ä¾è³´: sentence-transformers æˆ– numpy")
        print("è«‹åŸ·è¡Œ: pip3 install -r requirements.txt")
        sys.exit(1)
    
    index = load_index()
    if not index:
        logger.warning("Index is empty or not found")
        return []
    
    logger.info(f"Searching for: '{query}' (threshold: {threshold})")
    
    # è¼‰å…¥æ¨¡å‹ï¼ˆåªåŠ è¼‰ä¸€æ¬¡ï¼‰
    try:
        model = SentenceTransformer(Config.SEMANTIC_MODEL)
        query_vec = model.encode(query, convert_to_tensor=False)  # è¿”å› numpy æ•¸çµ„
    except Exception as e:
        logger.error(f"Failed to encode query: {e}")
        return []
    
    results = []
    embedding_dir = Config.EMBEDDINGS_DIR
    cache_hits = 0
    cache_misses = 0
    
    for mem in index.get('memories', []):
        try:
            mem_id = mem.get('id')
            
            # å„ªå…ˆä½¿ç”¨é å­˜çš„å‘é‡æ–‡ä»¶ï¼ˆå¿«é€Ÿè·¯å¾‘ï¼‰
            embedding_path = embedding_dir / f"mem_{mem_id}.npy"
            if embedding_path.exists():
                try:
                    mem_vec = np.load(embedding_path)
                    cache_hits += 1
                    # è½‰æ›ç‚º tensor è¨ˆç®—ç›¸ä¼¼åº¦
                    from sentence_transformers import util
                    score = float(util.cos_sim(query_vec, mem_vec)[0][0])
                except Exception as e:
                    # å‘é‡æ–‡ä»¶æå£ï¼Œå›é€€åˆ°é‡æ–°ç·¨ç¢¼
                    logger.warning(f"Corrupted embedding for memory #{mem_id}, regenerating...")
                    mem_vec = model.encode(mem.get('content', ''), convert_to_tensor=False)
                    score = float(util.cos_sim(query_vec, mem_vec)[0][0])
            else:
                # å‘é‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¾å ´ç·¨ç¢¼ï¼ˆä¸¦ä¿å­˜ä»¥ä¾›ä¸‹æ¬¡ä½¿ç”¨ï¼‰
                cache_misses += 1
                mem_content = mem.get('content', '')
                mem_vec = model.encode(mem_content, convert_to_tensor=False)
                
                # å˜—è©¦ä¿å­˜å‘é‡ï¼ˆéé—œéµæ“ä½œï¼Œå¤±æ•—ä¸å½±éŸ¿æœå°‹ï¼‰
                try:
                    embedding_dir.mkdir(parents=True, exist_ok=True)
                    np.save(embedding_path, mem_vec)
                except Exception as e:
                    logger.debug(f"Failed to save embedding for memory #{mem_id}: {e}")
                
                from sentence_transformers import util
                score = float(util.cos_sim(query_vec, mem_vec)[0][0])
            
            if score >= threshold:
                results.append({
                    'content': mem.get('content', '')[:100],
                    'state': mem.get('state', 'unknown'),
                    'importance': mem.get('current_importance', 0),
                    'score': score
                })
        except Exception as e:
            logger.debug(f"Error processing memory #{mem.get('id')}: {e}")
            continue
    
    # æ’åºä¸¦è¿”å› top_k
    results.sort(key=lambda x: x['score'], reverse=True)
    results = results[:top_k]
    
    logger.info(f"Found {len(results)} results (cache hits: {cache_hits}, misses: {cache_misses})")
    return results

def main():
    parser = argparse.ArgumentParser(description="ç¥ç³»çµ± - è¨˜æ†¶æª¢ç´¢")
    parser.add_argument("query", nargs="+", help="æœå°‹æŸ¥è©¢")
    parser.add_argument("--top-k", type=int, default=5, help="è¿”å›çµæœæ•¸é‡")
    parser.add_argument("--threshold", type=float, default=0.5, help="ç›¸ä¼¼åº¦é–¾å€¼")
    
    args = parser.parse_args()
    query = " ".join(args.query)
    
    print(f"\nğŸ” æœå°‹è¨˜æ†¶: '{query}'\n")
    
    results = search_memories(query, args.top_k, args.threshold)
    
    if not results:
        print("âŒ æœªæ‰¾åˆ°ç›¸é—œè¨˜æ†¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(results)} æ¢ç›¸é—œè¨˜æ†¶\n")
    for i, mem in enumerate(results, 1):
        print(f"{i}. [ç›¸ä¼¼åº¦ {mem['score']:.2f}] {mem['content']}...")
        print(f"   ç‹€æ…‹: {mem['state']} | é‡è¦æ€§: {mem['importance']:.2f}\n")

if __name__ == "__main__":
    main()
